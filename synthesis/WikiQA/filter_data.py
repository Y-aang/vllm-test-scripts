import pickle
from vllm import LLM, SamplingParams
from tqdm import tqdm

# åˆå§‹åŒ– LLM å’Œ tokenizerï¼ˆå‚è€ƒ download_wikiqa.py ä¸­çš„æ³¨é‡Šå†™æ³•ï¼‰
llm = LLM(model="meta-llama/Llama-3.2-3B", 
          gpu_memory_utilization=0.9,
          max_model_len=2000, 
          block_size=16, 
          disable_sliding_window=True, 
          enable_prefix_caching=True
        )
tokenizer = llm.get_tokenizer()

# è¯»å– pickle æ–‡ä»¶ï¼ˆå‚è€ƒ view_pickle.py çš„å†™æ³•ï¼‰
data_file = "wikiqa_doc_query_dict.pkl"
# data_file = "/home/shenyang/data/doc_query_dict.pkl"
with open(data_file, "rb") as pfile:
    doc_query_dict = pickle.load(pfile)

# éå†æ‰€æœ‰ keyï¼ˆpassagesï¼‰ï¼Œè®¡ç®—æ¯ä¸ª passage çš„ token é•¿åº¦
token_lengths = []
passage_token_pairs = []  # å­˜å‚¨ (passage, token_length) å¯¹
for passage in tqdm(doc_query_dict.keys()):
    # å‚è€ƒ download_wikiqa.py ä¸­æ³¨é‡Šçš„å†™æ³•è®¡ç®— token é•¿åº¦
    num_tokens = tokenizer(passage, return_tensors="pt")["input_ids"].shape[1]
    token_lengths.append(num_tokens)
    passage_token_pairs.append((passage, num_tokens))

# è®¡ç®—å¹³å‡å€¼
if token_lengths:
    avg_length = sum(token_lengths) / len(token_lengths)
    print(f"ğŸ“Œ æ€» passage æ•°é‡: {len(token_lengths)}")
    print(f"ğŸ“Œ Token é•¿åº¦å¹³å‡å€¼: {avg_length:.2f}")
    print(f"ğŸ“Œ Token é•¿åº¦æœ€å°å€¼: {min(token_lengths)}")
    print(f"ğŸ“Œ Token é•¿åº¦æœ€å¤§å€¼: {max(token_lengths)}")
    
    # æ‰¾åˆ°æœ€çŸ­çš„ wiki
    min_token_length = min(token_lengths)
    shortest_passage = min(passage_token_pairs, key=lambda x: x[1])[0]
    print(f"\nğŸ“Œ æœ€çŸ­çš„ wiki (Token é•¿åº¦: {min_token_length}):")
    print("=" * 80)
    print(shortest_passage)
    print("=" * 80)
else:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• passage æ•°æ®")

